{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video-Audio Emotion Congruence Detection - GPU Training\n",
    "\n",
    "This notebook trains the multimodal emotion recognition model on Google Colab with GPU support.\n",
    "\n",
    "**Author**: Rohan Jain  \n",
    "**GitHub**: https://github.com/Rohanjain2312/video-audio-emotion-congruence\n",
    "\n",
    "---\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Enable GPU**: Runtime → Change runtime type → GPU (T4 or better)\n",
    "2. **Upload Kaggle Credentials**: For CREMA-D dataset\n",
    "3. **Run all cells** in order\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"⚠️ WARNING: GPU not available. Enable GPU in Runtime settings!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/Rohanjain2312/video-audio-emotion-congruence.git\n",
    "%cd video-audio-emotion-congruence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setup Kaggle Credentials\n",
    "\n",
    "For CREMA-D dataset access:\n",
    "1. Download `kaggle.json` from https://www.kaggle.com/settings\n",
    "2. Upload it using the file browser (left sidebar)\n",
    "3. Run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup Kaggle credentials\n",
    "kaggle_dir = Path.home() / '.kaggle'\n",
    "kaggle_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Check if kaggle.json exists in current directory\n",
    "if Path('kaggle.json').exists():\n",
    "    !cp kaggle.json ~/.kaggle/\n",
    "    !chmod 600 ~/.kaggle/kaggle.json\n",
    "    print(\"✓ Kaggle credentials configured\")\n",
    "else:\n",
    "    print(\"⚠️ WARNING: kaggle.json not found. Upload it to access CREMA-D dataset.\")\n",
    "    print(\"Download from: https://www.kaggle.com/settings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Download Datasets\n",
    "\n",
    "This will download:\n",
    "- **RAVDESS**: All 24 actors (~1GB)\n",
    "- **CREMA-D**: Full dataset (~4GB)\n",
    "\n",
    "**Total time**: 15-30 minutes depending on connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download RAVDESS (audio)\n",
    "print(\"Downloading RAVDESS audio...\")\n",
    "!python src/data/download_datasets.py --dataset ravdess --gpu_mode\n",
    "\n",
    "print(\"\\nDownloading RAVDESS videos...\")\n",
    "!python src/data/download_videos.py --gpu_mode\n",
    "\n",
    "print(\"\\nDownloading CREMA-D...\")\n",
    "!python src/data/download_datasets.py --dataset cremad --gpu_mode\n",
    "\n",
    "print(\"\\n✓ All datasets downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verify Dataset Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Check RAVDESS\n",
    "ravdess_path = Path(\"data/raw/RAVDESS\")\n",
    "ravdess_videos = list(ravdess_path.rglob(\"*.mp4\"))\n",
    "ravdess_audio = list(ravdess_path.rglob(\"*.wav\"))\n",
    "\n",
    "print(\"RAVDESS:\")\n",
    "print(f\"  Videos: {len(ravdess_videos)}\")\n",
    "print(f\"  Audio: {len(ravdess_audio)}\")\n",
    "\n",
    "# Check CREMA-D\n",
    "cremad_path = Path(\"data/raw/CREMA-D\")\n",
    "if cremad_path.exists():\n",
    "    cremad_videos = list(cremad_path.rglob(\"*.flv\"))\n",
    "    cremad_audio = list(cremad_path.rglob(\"*.wav\"))\n",
    "    \n",
    "    print(\"\\nCREMA-D:\")\n",
    "    print(f\"  Videos: {len(cremad_videos)}\")\n",
    "    print(f\"  Audio: {len(cremad_audio)}\")\n",
    "else:\n",
    "    print(\"\\n⚠️ CREMA-D not found. Continuing with RAVDESS only.\")\n",
    "\n",
    "print(\"\\n✓ Dataset verification complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Preprocess Data\n",
    "\n",
    "Creates metadata files and train/val/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/data/preprocess.py\n",
    "\n",
    "print(\"\\n✓ Data preprocessing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Verify Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test of data loaders\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "\n",
    "from data.dataset_loaders import get_dataloaders\n",
    "\n",
    "print(\"Testing data loaders...\")\n",
    "train_loader, val_loader, test_loader = get_dataloaders(\n",
    "    \"data/processed/train_metadata.csv\",\n",
    "    \"data/processed/val_metadata.csv\",\n",
    "    \"data/processed/test_metadata.csv\",\n",
    "    batch_size=4,\n",
    "    mode='both'\n",
    ")\n",
    "\n",
    "# Get one batch\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"\\n✓ Data loaders working\")\n",
    "print(f\"  Video shape: {batch['video'].shape}\")\n",
    "print(f\"  Audio shape: {batch['audio'].shape}\")\n",
    "print(f\"  Batch size: {batch['video'].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train Multimodal Model\n",
    "\n",
    "**Training configuration**:\n",
    "- Epochs: 20\n",
    "- Batch size: 16\n",
    "- Learning rate: 1e-4\n",
    "- Frozen backbones (VideoMAE + Wav2Vec2)\n",
    "\n",
    "**Expected time**: 2-4 hours on T4 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multimodal model\n",
    "!python src/training/train_multimodal.py \\\n",
    "    --data_dir ./data/processed \\\n",
    "    --checkpoint_dir ./checkpoints/multimodal \\\n",
    "    --num_epochs 20 \\\n",
    "    --batch_size 16 \\\n",
    "    --learning_rate 1e-4\n",
    "\n",
    "print(\"\\n✓ Multimodal training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train Baseline Models (Optional)\n",
    "\n",
    "Train video-only and audio-only baselines for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train video-only baseline\n",
    "print(\"Training Video-Only baseline...\")\n",
    "!python src/training/train_video_only.py \\\n",
    "    --data_dir ./data/processed \\\n",
    "    --checkpoint_dir ./checkpoints/video_only \\\n",
    "    --num_epochs 20 \\\n",
    "    --batch_size 16\n",
    "\n",
    "print(\"\\n✓ Video-only training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train audio-only baseline\n",
    "print(\"Training Audio-Only baseline...\")\n",
    "!python src/training/train_audio_only.py \\\n",
    "    --data_dir ./data/processed \\\n",
    "    --checkpoint_dir ./checkpoints/audio_only \\\n",
    "    --num_epochs 20 \\\n",
    "    --batch_size 16\n",
    "\n",
    "print(\"\\n✓ Audio-only training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate multimodal model\n",
    "print(\"Evaluating Multimodal model...\")\n",
    "!python src/evaluation/evaluate.py \\\n",
    "    --checkpoint checkpoints/multimodal/best_model.pth \\\n",
    "    --model_type multimodal \\\n",
    "    --data_dir ./data/processed \\\n",
    "    --output_dir ./outputs/metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate video-only baseline (if trained)\n",
    "print(\"Evaluating Video-Only model...\")\n",
    "!python src/evaluation/evaluate.py \\\n",
    "    --checkpoint checkpoints/video_only/best_model.pth \\\n",
    "    --model_type video_only \\\n",
    "    --data_dir ./data/processed \\\n",
    "    --output_dir ./outputs/metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate audio-only baseline (if trained)\n",
    "print(\"Evaluating Audio-Only model...\")\n",
    "!python src/evaluation/evaluate.py \\\n",
    "    --checkpoint checkpoints/audio_only/best_model.pth \\\n",
    "    --model_type audio_only \\\n",
    "    --data_dir ./data/processed \\\n",
    "    --output_dir ./outputs/metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Compare All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "!python src/evaluation/compare_baselines.py \\\n",
    "    --metrics_dir ./outputs/metrics \\\n",
    "    --output_dir ./outputs/comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display evaluation results\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Load multimodal metrics\n",
    "metrics_path = Path(\"outputs/metrics/multimodal_metrics.json\")\n",
    "if metrics_path.exists():\n",
    "    with open(metrics_path, 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MULTIMODAL MODEL RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    em = metrics['emotion_metrics']\n",
    "    print(f\"\\nEmotion Recognition:\")\n",
    "    print(f\"  Accuracy:         {em['accuracy']:.4f}\")\n",
    "    print(f\"  Macro F1:         {em['macro_f1']:.4f}\")\n",
    "    print(f\"  Weighted F1:      {em['weighted_f1']:.4f}\")\n",
    "    \n",
    "    if 'congruence_metrics' in metrics:\n",
    "        cm = metrics['congruence_metrics']\n",
    "        print(f\"\\nCongruence Detection:\")\n",
    "        print(f\"  Accuracy:         {cm['accuracy']:.4f}\")\n",
    "        print(f\"  F1:               {cm['f1']:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "else:\n",
    "    print(\"Metrics file not found. Run evaluation first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comparison table\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "comparison_path = Path(\"outputs/comparisons/overall_comparison.csv\")\n",
    "if comparison_path.exists():\n",
    "    df = pd.read_csv(comparison_path)\n",
    "    print(\"\\nModel Comparison:\")\n",
    "    print(df.to_string(index=False))\n",
    "else:\n",
    "    print(\"Comparison file not found. Run comparison script first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Display Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from pathlib import Path\n",
    "\n",
    "# Display confusion matrix\n",
    "confusion_matrix_path = Path(\"outputs/metrics/multimodal_emotion_confusion_matrix.png\")\n",
    "if confusion_matrix_path.exists():\n",
    "    print(\"Emotion Confusion Matrix:\")\n",
    "    display(Image(filename=str(confusion_matrix_path)))\n",
    "else:\n",
    "    print(\"Confusion matrix not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display per-class metrics\n",
    "per_class_path = Path(\"outputs/metrics/multimodal_per_class_metrics.png\")\n",
    "if per_class_path.exists():\n",
    "    print(\"Per-Class Metrics:\")\n",
    "    display(Image(filename=str(per_class_path)))\n",
    "else:\n",
    "    print(\"Per-class metrics plot not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comparison plot\n",
    "comparison_plot_path = Path(\"outputs/comparisons/overall_comparison.png\")\n",
    "if comparison_plot_path.exists():\n",
    "    print(\"Model Comparison:\")\n",
    "    display(Image(filename=str(comparison_plot_path)))\n",
    "else:\n",
    "    print(\"Comparison plot not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference on a sample video\n",
    "from pathlib import Path\n",
    "\n",
    "# Find a sample video\n",
    "sample_videos = list(Path(\"data/raw/RAVDESS\").rglob(\"*.mp4\"))\n",
    "\n",
    "if sample_videos:\n",
    "    test_video = str(sample_videos[0])\n",
    "    print(f\"Testing inference on: {test_video}\\n\")\n",
    "    \n",
    "    !python src/inference/inference_pipeline.py \\\n",
    "        --checkpoint checkpoints/multimodal/best_model.pth \\\n",
    "        --video \"$test_video\"\n",
    "else:\n",
    "    print(\"No sample videos found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Download Trained Models\n",
    "\n",
    "Download checkpoints to your local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip checkpoints for download\n",
    "!zip -r checkpoints.zip checkpoints/\n",
    "!zip -r outputs.zip outputs/\n",
    "\n",
    "print(\"\\n✓ Files zipped and ready for download\")\n",
    "print(\"Download from the Files panel (left sidebar):\")\n",
    "print(\"  - checkpoints.zip (trained models)\")\n",
    "print(\"  - outputs.zip (metrics and visualizations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Mount Google Drive (Optional)\n",
    "\n",
    "Save checkpoints directly to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copy checkpoints to Drive\n",
    "!mkdir -p \"/content/drive/MyDrive/emotion-model-checkpoints\"\n",
    "!cp -r checkpoints/* \"/content/drive/MyDrive/emotion-model-checkpoints/\"\n",
    "!cp -r outputs/* \"/content/drive/MyDrive/emotion-model-checkpoints/\"\n",
    "\n",
    "print(\"✓ Checkpoints saved to Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Training complete! You have:\n",
    "\n",
    "✅ Downloaded and preprocessed datasets  \n",
    "✅ Trained multimodal model (and optional baselines)  \n",
    "✅ Evaluated performance with comprehensive metrics  \n",
    "✅ Generated visualizations and comparisons  \n",
    "✅ Saved checkpoints for deployment\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Download checkpoints** using the zip files created above\n",
    "2. **Deploy to Hugging Face Spaces** using the Gradio app\n",
    "3. **Share results** on GitHub with updated README\n",
    "\n",
    "---\n",
    "\n",
    "**Project**: [Video-Audio Emotion Congruence](https://github.com/Rohanjain2312/video-audio-emotion-congruence)  \n",
    "**Author**: [Rohan Jain](https://www.linkedin.com/in/jaroh23/)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}